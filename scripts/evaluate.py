"""
Evaluate trained CycleGAN model using FID score and other metrics.

This script provides evaluation utilities for assessing the quality
of generated Monet-style images.
"""

import argparse
import os
import numpy as np
import tensorflow as tf
from pathlib import Path
import sys

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.monetai.models import CycleGAN
from src.monetai.data import load_image_dataset
from src.monetai.utils import setup_gpu, save_generated_images


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Evaluate CycleGAN model performance')
    
    parser.add_argument('--model-path', type=str, required=True,
                       help='Path to trained model checkpoint')
    parser.add_argument('--test-photos', type=str, required=True,
                       help='Directory containing test photos')
    parser.add_argument('--real-monet', type=str, required=True,
                       help='Directory containing real Monet paintings')
    parser.add_argument('--output-dir', type=str, default='evaluation_results',
                       help='Directory to save evaluation results')
    parser.add_argument('--batch-size', type=int, default=1,
                       help='Batch size for evaluation')
    parser.add_argument('--image-size', type=int, default=256,
                       help='Image size for processing')
    
    return parser.parse_args()


def calculate_fid_score(real_dir: str, generated_dir: str) -> float:
    """
    Calculate FID score between real and generated images.
    
    This is a simplified placeholder. In practice, you would use
    the pytorch-fid library or implement the full FID calculation.
    """
    try:
        # Placeholder for FID calculation
        # In a real implementation, you would use pytorch-fid or similar
        print("FID calculation would be performed here...")
        print("For accurate FID scores, please use pytorch-fid library:")
        print(f"python -m pytorch_fid {real_dir} {generated_dir}")
        return 0.0
    except Exception as e:
        print(f"Error calculating FID: {e}")
        return float('inf')


def evaluate_image_quality(generated_dir: str) -> dict:
    """
    Evaluate basic image quality metrics.
    
    Args:
        generated_dir: Directory containing generated images
        
    Returns:
        Dictionary with quality metrics
    """
    metrics = {
        'num_images': 0,
        'avg_brightness': 0.0,
        'avg_contrast': 0.0,
        'color_diversity': 0.0
    }
    
    image_files = [f for f in os.listdir(generated_dir) 
                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    
    if not image_files:
        return metrics
    
    metrics['num_images'] = len(image_files)
    
    brightness_values = []
    contrast_values = []
    
    for img_file in image_files:
        img_path = os.path.join(generated_dir, img_file)
        try:
            # Load and analyze image
            image = tf.keras.preprocessing.image.load_img(img_path)
            img_array = tf.keras.preprocessing.image.img_to_array(image) / 255.0
            
            # Calculate brightness (mean pixel value)
            brightness = tf.reduce_mean(img_array).numpy()
            brightness_values.append(brightness)
            
            # Calculate contrast (standard deviation)
            contrast = tf.math.reduce_std(img_array).numpy()
            contrast_values.append(contrast)
            
        except Exception as e:
            print(f"Error processing {img_file}: {e}")
            continue
    
    if brightness_values:
        metrics['avg_brightness'] = np.mean(brightness_values)
        metrics['avg_contrast'] = np.mean(contrast_values)
        metrics['color_diversity'] = np.std(brightness_values)
    
    return metrics


def main():
    """Main evaluation function."""
    args = parse_args()
    
    # Setup GPU
    setup_gpu()
    
    # Validate input directories
    for path in [args.test_photos, args.real_monet]:
        if not os.path.exists(path):
            print(f"Error: Directory does not exist: {path}")
            return
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Initialize model
    print("Initializing CycleGAN model...")
    model = CycleGAN(
        img_height=args.image_size,
        img_width=args.image_size
    )
    
    # Load model checkpoint
    print(f"Loading model from: {args.model_path}")
    if not model.load_model(args.model_path):
        print("Error: Could not load model checkpoint")
        return
    
    print("‚úì Model loaded successfully")
    
    # Load test dataset
    print(f"Loading test images from: {args.test_photos}")
    try:
        test_dataset = load_image_dataset(
            args.test_photos,
            img_height=args.image_size,
            img_width=args.image_size,
            batch_size=args.batch_size
        )
        print("‚úì Test images loaded")
    except Exception as e:
        print(f"Error loading test images: {e}")
        return
    
    # Generate images for evaluation
    generated_dir = os.path.join(args.output_dir, 'generated_monet')
    print(f"Generating images for evaluation...")
    
    try:
        num_generated = save_generated_images(
            dataset=test_dataset,
            generator=model.generator_g,
            output_dir=generated_dir,
            prefix="eval_monet"
        )
        print(f"‚úì Generated {num_generated} images for evaluation")
    except Exception as e:
        print(f"Error generating images: {e}")
        return
    
    # Evaluate image quality
    print("Evaluating image quality metrics...")
    quality_metrics = evaluate_image_quality(generated_dir)
    
    print("\n" + "="*50)
    print("EVALUATION RESULTS")
    print("="*50)
    print(f"Number of generated images: {quality_metrics['num_images']}")
    print(f"Average brightness: {quality_metrics['avg_brightness']:.4f}")
    print(f"Average contrast: {quality_metrics['avg_contrast']:.4f}")
    print(f"Color diversity: {quality_metrics['color_diversity']:.4f}")
    
    # Calculate FID score (placeholder)
    print("\nCalculating FID score...")
    fid_score = calculate_fid_score(args.real_monet, generated_dir)
    if fid_score != float('inf'):
        print(f"FID Score: {fid_score:.2f}")
    
    # Save evaluation report
    report_file = os.path.join(args.output_dir, 'evaluation_report.txt')
    with open(report_file, 'w') as f:
        f.write("CycleGAN Model Evaluation Report\n")
        f.write("="*40 + "\n\n")
        f.write(f"Model: {args.model_path}\n")
        f.write(f"Test Images: {args.test_photos}\n")
        f.write(f"Reference Images: {args.real_monet}\n\n")
        f.write("Quality Metrics:\n")
        f.write(f"- Number of images: {quality_metrics['num_images']}\n")
        f.write(f"- Average brightness: {quality_metrics['avg_brightness']:.4f}\n")
        f.write(f"- Average contrast: {quality_metrics['avg_contrast']:.4f}\n")
        f.write(f"- Color diversity: {quality_metrics['color_diversity']:.4f}\n")
        if fid_score != float('inf'):
            f.write(f"- FID Score: {fid_score:.2f}\n")
    
    print(f"\nüìä Evaluation report saved to: {report_file}")
    print(f"üñºÔ∏è  Generated images saved to: {generated_dir}")
    
    # Generate sample comparisons
    print("\nGenerating sample comparisons...")
    comparison_dir = os.path.join(args.output_dir, 'comparisons')
    os.makedirs(comparison_dir, exist_ok=True)
    
    # Generate a few comparison images
    for i, batch in enumerate(test_dataset.take(5)):
        output_path = os.path.join(comparison_dir, f'comparison_{i:03d}.png')
        model.generate_images(batch, save_path=output_path)
    
    print(f"‚úì Sample comparisons saved to: {comparison_dir}")
    print("\nüéØ Evaluation completed!")


if __name__ == '__main__':
    main()
